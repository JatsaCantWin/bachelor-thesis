\chapter{Comparison} 

\par When comparing the applicability of various technologies, it is vital first to establish the needs of the system that will utilize them. The system that concerns us that is, the energy cluster, gathers data from various member clients and sends it to a database to process said data. While the size of an energy cluster is constrained by the area it can at most occupy according to the law, the number of energy cluster members is virtually limitless. Some small clusters can feature around ten members, while larger clusters can simultaneously serve hundreds or even thousands of members. 
\par It is also often the case that initially, small energy clusters grow into larger entities, producing more significant amounts of data and needing more processing power to analyze them. Sometimes, even a very efficient and powerful database cannot handle the needs of an entire cluster alone. Data may need to be compartmentalized into different nodes, each of which has to handle different kinds or different sources of requests. Databases can also scale with equipment - a more robust server infrastructure can handle more requests even more quickly. This type of scaling by upgrading equipment is called \textbf{vertical scaling}, while improving the system by expanding the amount of equipment handling data is called \textbf{horizontal scaling}. SQL-type solutions usually scale vertically, while NoSQL solutions prefer horizontal scaling\citep{SQLvNoSQLvNewSQL}.
\par Another problem that may befall energy cluster databases is the significantly lowered performance of the entire database while analyzing a large subset of data. This problem can be remedied by implementing a separate instance of a database that may even utilize a different DBMS to perform all the necessary analyses on the data copied into that additional instance.

\section{Using query languages to perform analyses}

\par It is not enough to simply aggregate data gathered by meters and then store it inside the database, allowing the user to read the current value of said data. Analyzing data, such as the overall energy produced/consumed within a certain period, the reactive energy being consumed both immediately and within a settlement period, and the overall quality of energy during a given week is necessary to calculate fines to be paid and alert the cluster member of dangerous irregularities in the quality of energy being produced/consumed.
\par DBMS Query Languages allow the creation of automated functions that can return data processed as needed. It would thus be helpful to design queries, statements, SQL Views, or functions that allow the user to access the following analyses quickly:
\begin{itemize}
  \item The difference between energy consumed and produced at any time window. Monitoring this data can be used to see whether the entity can satisfy its own energy needs or whether it needs to import energy from outside the grid.
  \item Energy consumption within a given time window, that exceeds contracted power. It should be monitored in real-time to inform the user about defects quickly.
  \item The energy produced/consumed balance within a settlement period. This measure has to be considered once per settlement. Since the energy meter gathers data about energy production and consumption as an overall tally and does not automatically reset once per settlement period, those values must be corrected by their value at the beginning of the settlement period.
  \item Reactive power and how much it exceeds the norm. Ideally, it should be monitored in real-time to alert the user about any irregularities.
  \item Qualitative data about the energy, such as the grid frequency or phase voltage. These have to be compared against the norms once per settlement period. 
\end{itemize}

\subsection{SQLite}
\par SQLite severely lacks in the area of data analytics. It allows the user to define views and functions - previously-stored series of queries and calculations that can be easily accessed by the user later.
\lstinputlisting[language=SQL, caption=Example implementation in SQLite]{src/SQLite.SQL}
\par Parametrized Views do not exist in SQLite in the form they exist in most other SQL implementations; this unfortuately means that it is impossible to create Views that are going to only show data relevant to a particular settlement period.

\subsection{PostgreSQL}
\par PostgreSQL has a wider range of capabilities and functionalities than SQLite, supporting more analytic functions. It also supports \textbf{PL/SQL} (Procedural Language/Structured Query Language). This procedural language provides a way to create SQL Queries in a manner similar to other procedural languages like C or Pascal. 
\lstinputlisting[language=SQL, caption=Example implementation in PostgreSQL]{src/PostgreSQL.SQL}

\subsection{Cassandra}
\par Cassandra does not support parametrized queries. Furthermore, it cannot pass arguments to Views as you can in SQLite. It does support parametrized queries listed below; these, however, cannot be in any way stored on the database itself. They have to be kept as prepared statements in an external application.

\subsection{MongoDB}
\par MongoDB does not support stored functions, procedures, or views like RDBMSs. Instead, MongoDB provides features like the MongoDB Aggregation Framework, allowing users to create a virtual collection that retrieves queried and processed data. 

\subsection{Prometheus}
\par Although Prometheus does not have traditional functions, views, or stored procedures, PromQL provides functions that can operate within its unique schema, to deliver all necessary analyses. 

\subsection{Triggers and alerts}
\par \textbf{Triggers} are pieces of code executed in response to a specific event in a database. They are commonly used to perform various tasks, such as validating data, updating other tables or collections, or sending notifications. In the context of energy clusters, these mechanisms can autofill capture timestamps or record irregular data gathered in real-time, such as energy consumption or reactive power.
\par Unlike other considered in this paper DBMS, Prometheus does not support any trigger mechanism. It does, however, support Alerts that can be used to inform the user about data outside the expected norms.
\par Triggers themselves cannot communicate in any way with any external systems that would alert the user about data that falls outside the allowed range. Data monitoring and visualization platforms, such as Grafana or Datadog, allow even the users of RDBMSs to draw graphs of each analysis and send notifications to users about deviations from the norm.
\begin{figure}[htbp]
 \centering
 \includegraphics[width=1\textwidth]{gfx/ochotnica-dolna-atende-energy-consumed-produced}
 \caption{Energy consumption/production measured at an Energy Cluster member in Ochotnica Dolna, as shown on Grafana}
\end{figure}
\begin{figure}[htbp]
 \centering
 \includegraphics[width=1\textwidth]{gfx/ochotnica-dolna-atende-frequency}
 \caption{Frequency monitored at an Energy Cluster member in Ochotnica Dolna, as shown on Grafana}
\end{figure}

\section{Performance analyses}

\par While all DBMSs can scale either vertically or horizontally. Every such improvement, however, is associated with higher costs. Performing basic \textbf{CRUD} operations is also essential in deciding on a specific DBMS since higher-performance databases require lower expenses when purchasing the hardware on which the database will operate.
\par For the purposes of this paper, python scripts capable of executing basic database operations, measuring the execution time, and drawing charts representing the results were written. 

\subsection{CRUD operations performing objects}
\par A python class was written for each considered DBMS to connect to these databases and perform basic operations on stored data. Every Class implementing this Interface must introduce four methods, each representing one of the four CRUD operations. Each of these methods must take two arguments: the amount of data created, read, updated, or deleted in a transaction or a batch operation and the number of transactions or batch operations performed. The Interface also defines a "reset()" operation that wipes leftover data from the database in preparation for performing the analysis and a "getName()" method that returns the name of the DBMS it performs the operations on, applicable when drawing charts.
\lstinputlisting[language=Python, caption=DatabaseTestingInterface.py]{src/DatabaseTestingInterface.py}
\par While not included inside the Interface definition, every database connection object should also define a constructor that establishes a connection with the database and processes external CSV files into data used in CRUD operations. They should also define a destructor that would finally sever the connection to the database upon the object's destruction. They also usually implement methods that help create necessary data storage structures, like tables or collections.
\lstinputlisting[language=Python, caption=SQLiteDatabaseTesting.py]{src/SQLiteDatabaseTesting.py}
\par The default SQLite interface is used to implement this Class. The constructor stores the connection to the database file, whose path was provided during the object's construction, and processes external CSV data into a dictionary containing lists of data values for each table separately. 
\par The structure of the tables that will contain these data entries is hardcoded because every data type is organized in a similar way.
\lstinputlisting[language=Python, caption=PostgresDatabaseTesting.py]{src/PostgreSQLDatabaseTesting.py}
\par The PostgreSQL implementation is very similar to the SQLite implementation since the PostgreSQL handling library "psycopg2" uses the same basic syntax as the sqlite3 library. The main difference is in the structure of the connection string - while SQLite requires the path to the location of the database on disk, PostgreSQL needs a string of data that defines the IP address of the DBMS service, the database name, the user name, and password. 
\lstinputlisting[language=Python, caption=CassandraDatabaseTesting.py]{src/CassandraDatabaseTesting.py}
\par The Cassandra version of this Class is also very similar to its SQL counterparts due to CQL's resemblance to SQL. The lack of transactions in Cassandra causes the main difference in implementation. Instead, the BatchStatement mechanism was used to perform multiple operations at once.
\lstinputlisting[language=Python, caption=MongoDatabaseTesting.py]{src/MongoDatabaseTesting.py}
\par The main difference in the MongoDB implementation is the use of documents to define data. There are no columns in document collections, so each document has to define the schema separately. The pymongo driver is also sensitive to the python data type of variables included in the document. Because of that, a method converting strings into timestamps was also defined within the Class.
\subsection{Evaluating performance}
\par A DatabaseTester Class was written to utilize these connection objects to measure the execution time of each operation. 
\lstinputlisting[language=Python, caption=DatabaseTester.py]{src/DatabaseTester.py}
\par The constructor of this object requires providing a list of DatabaseCRUDConnection objects that will be used when performing the analysis. The "executionTimeTable" field provides storage for analysis results. There are four methods used to measure the execution time of basic CRUD operations. Each of these four methods requires two arguments: the number of records to be operated on and the number of transactions/batch operations used to perform said operations. The operation execution time is measured using the timeit library, and the result is recorded in the appropriate place inside the "executionTimeTable" field. The "drawGraphs()" method draws bar charts based on data stored inside the "executionTimeTable()" field.
\par This code was used to perform a performance analysis, with all database instances initiated on the same local machine, using docker virtualization. 
\begin{figure}[htbp]
 \centering
 \includegraphics[width=1\textwidth]{gfx/create}
 \caption{Time elapsed while creating a 1000 records}
\end{figure}
\begin{figure}[htbp]
 \centering
 \includegraphics[width=1\textwidth]{gfx/read}
 \caption{Time elapsed while reading a 1000 records}
\end{figure}
\begin{figure}[htbp]
 \centering
 \includegraphics[width=1\textwidth]{gfx/update}
 \caption{Time elapsed while updating a 1000 records}
\end{figure}
\begin{figure}[htbp]
 \centering
 \includegraphics[width=1\textwidth]{gfx/delete}
 \caption{Time elapsed while deleting a 1000 records}
\end{figure}
\par Data for Cassandra's batch read queries was not measured, since the amount of information downloaded, quickly overwhelmed the maximum size of a batch operation permited by Cassandra.
\par The results suggest that database performance can usually be improved, in many cases considerably, by sending data in batches. If lower-performing database equipment struggles to meet the demands of a large Energy Cluster, it may thus collect data at longer intervals.
\par PostgreSQL, because of its complexity, was significantly less performant than other DBMSs. Such is the price for checking consistency and its robust data analytics, and constraint system.
\par Because of its paradigm's uniqueness, it was impossible to compare Prometheus to other databases. Prometheus does include the option to tune the data-scraping interval. Prometheus has been proven to handle large amounts of data inserts every second, enough to satisfy the needs of the largest Energy Clusters. \citep{prometheusSoundcloud}