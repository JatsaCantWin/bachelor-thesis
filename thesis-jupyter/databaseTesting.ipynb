{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Tuple, Union\n",
    "import csv\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "class DatabaseTestingInterface (ABC):\n",
    "    @abstractmethod\n",
    "    def create(self, rows_created: int = 1, transactions: int = 1):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read(self, rows_read: int = 1, transactions: int = 1):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, rows_updated: int = 1, transactions: int = 1):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def delete(self, rows_deleted: int = 1, transactions: int = 1):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getName(self) -> str:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def get_csv_file_paths(folder_path: str) -> List[str]:\n",
    "    return glob.glob(os.path.join(folder_path, \"*.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class SqliteDatabaseTesting(DatabaseTestingInterface):\n",
    "    def __init__(self, connection_string: str, csv_file_paths: List[str]):\n",
    "        self.connection = sqlite3.connect(connection_string)\n",
    "        self.csv_data = {}\n",
    "        self.table_names = []\n",
    "        self.table_column_names = {}\n",
    "\n",
    "        for file_path in csv_file_paths:\n",
    "            table_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            self.table_names.append(table_name)\n",
    "            self.table_column_names[table_name], self.csv_data[table_name] = self.__parse_csv_data(file_path)\n",
    "\n",
    "        self.__normalize_table_column_names()\n",
    "        self.__create_tables()\n",
    "        self.connection.commit()\n",
    "\n",
    "    def __create_tables(self):\n",
    "        c = self.connection.cursor()\n",
    "        for table_name in self.table_names:\n",
    "            c.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            column_definitions = []\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][0]} timestamp NOT NULL\")\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][1]} timestamp NOT NULL\")\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][2]} numerical NOT NULL\")\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][3]} int\")\n",
    "            column_definitions_str = \", \".join(column_definitions)\n",
    "            c.execute(f\"CREATE TABLE {table_name} ({column_definitions_str})\")\n",
    "        self.connection.commit()\n",
    "\n",
    "    def __normalize_table_column_names(self):\n",
    "        for table_name, column_names in self.table_column_names.items():\n",
    "            column_names = [''.join(c for c in name.strip() if c.isalnum() or c == '_') for name in column_names]\n",
    "            self.table_column_names[table_name] = column_names\n",
    "\n",
    "    def __parse_csv_data(self, file_path: str) -> Tuple[List[str], List[Tuple[str, str, float, int]]]:\n",
    "        MAX_DATA_READ = 1000\n",
    "        with open(file_path, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"|\")\n",
    "            column_names = next(reader)\n",
    "            rows = itertools.islice(reader, MAX_DATA_READ)\n",
    "            data = [tuple(row) for row in rows]\n",
    "        return column_names, data\n",
    "\n",
    "    def create(self, rows_created: int = 1, transactions: int = 1):\n",
    "        for table_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                c = self.connection.cursor()\n",
    "                for i in range(rows_created):\n",
    "                    c.execute(f\"INSERT INTO {table_name} VALUES (?, ?, ?, ?)\", data[i+transaction*rows_created])\n",
    "                self.connection.commit()\n",
    "\n",
    "    def read(self, rows_read: int = 1, transactions: int = 1):\n",
    "        for table_name in self.table_names:\n",
    "            for transaction in range(transactions):\n",
    "                c = self.connection.cursor()\n",
    "                c.execute(f\"SELECT * FROM {table_name} LIMIT {rows_read}\")\n",
    "                c.fetchone()\n",
    "\n",
    "    def update(self, rows_updated: int = 1, transactions: int = 1):\n",
    "        for table_name in self.table_names:\n",
    "            for transaction in range(transactions):\n",
    "                c = self.connection.cursor()\n",
    "                for i in range(rows_updated):\n",
    "                    c.execute(f\"UPDATE {table_name} SET {self.table_column_names[table_name][2]} = 0 WHERE {self.table_column_names[table_name][0]} = \\\"{self.csv_data[table_name][i+transaction*rows_updated][0]}\\\"\")\n",
    "                self.connection.commit()\n",
    "\n",
    "    def delete(self, rows_deleted: int = 1, transactions: int = 1):\n",
    "        for table_name in self.table_names:\n",
    "            for transaction in range(transactions):\n",
    "                c = self.connection.cursor()\n",
    "                for i in range(rows_deleted):\n",
    "                    c.execute(f\"DELETE FROM {table_name} WHERE {self.table_column_names[table_name][0]} = \\\"{self.csv_data[table_name][i+transaction*rows_deleted][0]}\\\"\")\n",
    "                self.connection.commit()\n",
    "\n",
    "    def reset(self):\n",
    "        c = self.connection.cursor()\n",
    "        c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        table_names = [row[0] for row in c.fetchall()]\n",
    "        for table_name in table_names:\n",
    "            c.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        self.connection.commit()\n",
    "        self.__create_tables()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.connection.close()\n",
    "\n",
    "    def getName(self) -> str:\n",
    "        return \"SQLite\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "class PostgresDatabaseTesting(DatabaseTestingInterface):\n",
    "    def __init__(self, connection_string: str, csv_file_paths: List[str]):\n",
    "        self.connection = psycopg2.connect(connection_string)\n",
    "        self.csv_data = {}\n",
    "        self.table_names = []\n",
    "        self.table_column_names = {}\n",
    "\n",
    "        for file_path in csv_file_paths:\n",
    "            table_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            self.table_names.append(table_name)\n",
    "            self.table_column_names[table_name], self.csv_data[table_name] = self.__parse_csv_data(file_path)\n",
    "\n",
    "        self.__normalize_table_column_names()\n",
    "        self.__create_tables()\n",
    "\n",
    "    def __create_tables(self):\n",
    "        cursor = self.connection.cursor()\n",
    "        for table_name in self.table_names:\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            column_definitions = []\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][0]} timestamp NOT NULL\")\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][1]} timestamp NOT NULL\")\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][2]} numeric NOT NULL\")\n",
    "            column_definitions.append(f\"{self.table_column_names[table_name][3]} integer\")\n",
    "            column_definitions_str = \", \".join(column_definitions)\n",
    "            cursor.execute(f\"CREATE TABLE {table_name} ({column_definitions_str})\")\n",
    "        self.connection.commit()\n",
    "\n",
    "    def __normalize_table_column_names(self):\n",
    "        for table_name, column_names in self.table_column_names.items():\n",
    "            column_names = [''.join(c for c in name.strip() if c.isalnum() or c == '_') for name in column_names]\n",
    "            self.table_column_names[table_name] = column_names\n",
    "\n",
    "    def __parse_csv_data(self, file_path: str) -> Tuple[List[str], List[Tuple[str, str, float, int]]]:\n",
    "        MAX_DATA_READ = 1000\n",
    "        with open(file_path, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"|\")\n",
    "            column_names = next(reader)\n",
    "            rows = itertools.islice(reader, MAX_DATA_READ)\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                row[0] = row[0].replace(\",\", \".\")\n",
    "                row[1] = row[1].replace(\",\", \".\")\n",
    "                data.append(tuple(row))\n",
    "\n",
    "        return column_names, data\n",
    "\n",
    "    def create(self, rows_created: int = 1, transactions: int = 1):\n",
    "        for table_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                cursor = self.connection.cursor()\n",
    "                for i in range(rows_created):\n",
    "                    cursor.execute(f\"INSERT INTO {table_name} VALUES (%s, %s, %s, %s)\", data[i+transaction*rows_created])\n",
    "                self.connection.commit()\n",
    "\n",
    "    def read(self, rows_read: int = 1, transactions: int = 1):\n",
    "        for table_name in self.table_names:\n",
    "            for transaction in range(transactions):\n",
    "                cursor = self.connection.cursor()\n",
    "                cursor.execute(f\"SELECT * FROM {table_name} LIMIT {rows_read}\")\n",
    "                cursor.fetchone()\n",
    "\n",
    "    def update(self, rows_updated: int = 1, transactions: int = 1):\n",
    "        for table_name in self.table_names:\n",
    "            for transaction in range(transactions):\n",
    "                cursor = self.connection.cursor()\n",
    "                for i in range(rows_updated):\n",
    "                    cursor.execute(f\"UPDATE {table_name} SET {self.table_column_names[table_name][2]} = 0 WHERE {self.table_column_names[table_name][0]} = %s\", (self.csv_data[table_name][i+transaction*rows_updated][0],))\n",
    "                self.connection.commit()\n",
    "\n",
    "    def delete(self, rows_deleted: int = 1, transactions: int = 1):\n",
    "        for table_name in self.table_names:\n",
    "            for transaction in range(transactions):\n",
    "                cursor = self.connection.cursor()\n",
    "                for i in range(rows_deleted):\n",
    "                    cursor.execute(f\"DELETE FROM {table_name} WHERE {self.table_column_names[table_name][0]} = %s\", (self.csv_data[table_name][i+transaction*rows_deleted][0],))\n",
    "                self.connection.commit()\n",
    "\n",
    "    def reset(self):\n",
    "        cursor = self.connection.cursor()\n",
    "        for table_name in self.table_names:\n",
    "            cursor.execute(f\"DROP TABLE {table_name}\")\n",
    "        self.connection.commit()\n",
    "        self.__create_tables()\n",
    "\n",
    "    def getName(self) -> str:\n",
    "        return \"PostgreSQL\"\n",
    "\n",
    "    def __del__(self):\n",
    "        self.connection.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import SimpleStatement, BatchStatement\n",
    "from typing import List, Tuple\n",
    "\n",
    "class CassandraDatabaseTesting(DatabaseTestingInterface):\n",
    "    def __init__(self, connection_string: str, csv_file_paths: List[str]):\n",
    "        self.cluster = Cluster([connection_string])\n",
    "        self.session = self.cluster.connect()\n",
    "        self.csv_data = {}\n",
    "        self.table_names = []\n",
    "        self.table_column_names = {}\n",
    "        self.KEYSPACE_NAME = \"databaseTesting\"\n",
    "\n",
    "        for file_path in csv_file_paths:\n",
    "            table_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            self.table_names.append(table_name)\n",
    "            self.table_column_names[table_name], self.csv_data[table_name] = self.__parse_csv_data(file_path)\n",
    "\n",
    "        self.__normalize_table_column_names()\n",
    "        self.__execute_simple_statement(f\"DROP KEYSPACE IF EXISTS {self.KEYSPACE_NAME}\")\n",
    "        self.__create_tables()\n",
    "\n",
    "    def __execute_simple_statement(self, query: str, parameters: Tuple = ()):\n",
    "        statement = SimpleStatement(query, consistency_level=cassandra.ConsistencyLevel.ONE)\n",
    "        self.session.execute(statement, parameters)\n",
    "\n",
    "    def __execute_batch_statement(self, statements: List[str], parameters_list: List[Tuple[Tuple]]):\n",
    "        batch = BatchStatement(consistency_level=cassandra.ConsistencyLevel.ONE)\n",
    "        for statement, parameters in zip(statements, parameters_list):\n",
    "            batch.add(statement, parameters)\n",
    "        self.session.execute(batch)\n",
    "\n",
    "    def __create_tables(self):\n",
    "        self.__execute_simple_statement(f\"CREATE KEYSPACE {self.KEYSPACE_NAME} WITH REPLICATION = {{'class': 'SimpleStrategy', 'replication_factor': 1}}\")\n",
    "        self.__execute_simple_statement(f\"USE {self.KEYSPACE_NAME}\")\n",
    "        for table_name in self.table_names:\n",
    "            self.__execute_simple_statement(f\"CREATE TABLE IF NOT EXISTS {table_name} ({self.table_column_names[table_name][0]} timestamp, {self.table_column_names[table_name][1]} timestamp, {self.table_column_names[table_name][2]} float, {self.table_column_names[table_name][3]} int, PRIMARY KEY ({self.table_column_names[table_name][0]}, {self.table_column_names[table_name][1]}))\")\n",
    "\n",
    "    def __normalize_table_column_names(self):\n",
    "        for table_name, column_names in self.table_column_names.items():\n",
    "            column_names = [''.join(c for c in name.strip() if c.isalnum() or c == '_') for name in column_names]\n",
    "            self.table_column_names[table_name] = column_names\n",
    "\n",
    "    def __parse_csv_data(self, file_path: str) -> Tuple[List[str], List[Tuple[str, str, float, int]]]:\n",
    "        MAX_DATA_READ = 1000\n",
    "        with open(file_path, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"|\")\n",
    "            column_names = next(reader)\n",
    "            rows = itertools.islice(reader, MAX_DATA_READ)\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                row[0] = row[0].replace(\",\", \".\")\n",
    "                row[1] = row[1].replace(\",\", \".\")\n",
    "                data.append(tuple(row))\n",
    "\n",
    "        return column_names, data\n",
    "\n",
    "    def create(self, rows_created: int = 1, transactions: int = 1):\n",
    "        for table_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                statements = []\n",
    "                parameters_list = []\n",
    "                for i in range(rows_created):\n",
    "                    statements.append(f\"INSERT INTO {table_name} ({self.table_column_names[table_name][0]}, {self.table_column_names[table_name][1]}, {self.table_column_names[table_name][2]}, {self.table_column_names[table_name][3]}) VALUES (%s, %s, %s, %s)\")\n",
    "                    parameters_list.append((data[i][0], data[i][1], float(data[i][2]), int(data[i][3])))\n",
    "\n",
    "                self.__execute_batch_statement(statements=statements, parameters_list=parameters_list)\n",
    "\n",
    "    def read(self, rows_read: int = 1, transactions: int = 1):\n",
    "        for table_name in self.table_names:\n",
    "            for transaction in range(transactions):\n",
    "                statement = f\"SELECT * FROM {table_name} LIMT {rows_read}\"\n",
    "                self.session.execute(statement)\n",
    "\n",
    "    def update(self, rows_updated: int = 1, transactions: int = 1):\n",
    "        for table_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                statements = []\n",
    "                parameters_list = []\n",
    "                for i in range(rows_updated):\n",
    "                    statements.append(f\"UPDATE {table_name} SET {self.table_column_names[table_name][2]} = %s WHERE {self.table_column_names[table_name][0]} = %s AND {self.table_column_names[table_name][1]} = %s\")\n",
    "                    parameters_list.append((float(self.csv_data[table_name][i][2]), self.csv_data[table_name][i][0], self.csv_data[table_name][i][1]))\n",
    "\n",
    "                self.__execute_batch_statement(statements=statements, parameters_list=parameters_list)\n",
    "\n",
    "    def delete(self, rows_deleted: int = 1, transactions: int = 1):\n",
    "        for table_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                statements = []\n",
    "                parameters_list = []\n",
    "                for i in range(rows_deleted):\n",
    "                    statements.append(f\"DELETE FROM {table_name} WHERE {self.table_column_names[table_name][0]} = %s AND {self.table_column_names[table_name][1]} = %s\")\n",
    "                    parameters_list.append((self.csv_data[table_name][i][0], self.csv_data[table_name][i][1]))\n",
    "\n",
    "                self.__execute_batch_statement(statements=statements, parameters_list=parameters_list)\n",
    "\n",
    "    def reset(self):\n",
    "        self.__execute_simple_statement(f\"DROP KEYSPACE {self.KEYSPACE_NAME}\")\n",
    "        self.__create_tables()\n",
    "\n",
    "    def getName(self) -> str:\n",
    "        return \"Cassandra\"\n",
    "\n",
    "    def __del__(self):\n",
    "        self.session.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.operations import InsertOne, UpdateOne, DeleteOne\n",
    "\n",
    "class MongoDatabaseTesting(DatabaseTestingInterface):\n",
    "    def __init__(self, connection_string: str, csv_file_paths: List[str]):\n",
    "        self.client = MongoClient(connection_string)\n",
    "        self.csv_data = {}\n",
    "        self.collection_names = []\n",
    "        self.collection_column_names = {}\n",
    "        self.DATABASE_NAME = \"databasetesting\"\n",
    "\n",
    "        for file_path in csv_file_paths:\n",
    "            collection_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            self.collection_names.append(collection_name)\n",
    "            self.collection_column_names[collection_name], self.csv_data[collection_name] = self.__parse_csv_data(file_path)\n",
    "\n",
    "        self.__normalize_collection_column_names()\n",
    "\n",
    "    def __normalize_collection_column_names(self):\n",
    "        for database_name, column_names in self.collection_column_names.items():\n",
    "            column_names = [''.join(c for c in name.strip() if c.isalnum() or c == '_') for name in column_names]\n",
    "            self.collection_column_names[database_name] = column_names\n",
    "\n",
    "    def __parse_csv_data(self, file_path: str) -> Tuple[List[str], List[Tuple[str, str, float, int]]]:\n",
    "        MAX_DATA_READ = 1000\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"|\")\n",
    "            column_names = next(reader)\n",
    "            rows = itertools.islice(reader, MAX_DATA_READ)\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                row[0] = row[0].replace(\",\", \".\")\n",
    "                row[1] = row[1].replace(\",\", \".\")\n",
    "                data.append(tuple(row))\n",
    "\n",
    "        return column_names, data\n",
    "\n",
    "    def __convert_timestamp(self, timestamp_string: str):\n",
    "        return datetime.strptime(timestamp_string, '%Y-%m-%d %H:%M:%S.%f').timestamp()\n",
    "\n",
    "    def create(self, rows_created: int = 1, transactions: int = 1):\n",
    "        for collection_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                requests = []\n",
    "                for i in range(rows_created):\n",
    "                    requests.append(InsertOne({\n",
    "                        self.collection_column_names[collection_name][0]: self.__convert_timestamp(data[i][0]),\n",
    "                        self.collection_column_names[collection_name][1]: self.__convert_timestamp(data[i][1]),\n",
    "                        self.collection_column_names[collection_name][2]: data[i][2],\n",
    "                        self.collection_column_names[collection_name][3]: data[i][3]\n",
    "                    }))\n",
    "                for collection_name in self.collection_names:\n",
    "                    db = self.client[self.DATABASE_NAME]\n",
    "                    collection = db[collection_name]\n",
    "                    collection.bulk_write(requests, ordered=False)\n",
    "\n",
    "    def read(self, rows_read: int = 1, transactions: int = 1):\n",
    "        for collection_name in self.collection_names:\n",
    "            db = self.client[self.DATABASE_NAME]\n",
    "            collection = db[collection_name]\n",
    "            for transaction in range(transactions):\n",
    "                collection.find().limit(rows_read).next()\n",
    "\n",
    "    def update(self, rows_updated: int = 1, transactions: int = 1):\n",
    "        for collection_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                requests = []\n",
    "                for i in range(rows_updated):\n",
    "                    requests.append(UpdateOne({self.collection_column_names[collection_name][0]: self.__convert_timestamp(self.csv_data[collection_name][i][0]), self.collection_column_names[collection_name][1]: self.__convert_timestamp(self.csv_data[collection_name][i][1])}, {'$set': {self.collection_column_names[collection_name][2]: 0}}))\n",
    "                for collection_name in self.collection_names:\n",
    "                    db = self.client[self.DATABASE_NAME]\n",
    "                    collection = db[collection_name]\n",
    "                    collection.bulk_write(requests, ordered=False)\n",
    "\n",
    "    def delete(self, rows_deleted: int = 1, transactions: int = 1):\n",
    "        for collection_name, data in self.csv_data.items():\n",
    "            for transaction in range(transactions):\n",
    "                requests = []\n",
    "                for i in range(rows_deleted):\n",
    "                    requests.append(DeleteOne({self.collection_column_names[collection_name][0]: self.__convert_timestamp(self.csv_data[collection_name][i][0]), self.collection_column_names[collection_name][1]: self.__convert_timestamp(self.csv_data[collection_name][i][1])}))\n",
    "                for collection_name in self.collection_names:\n",
    "                    db = self.client[self.DATABASE_NAME]\n",
    "                    collection = db[collection_name]\n",
    "                    collection.bulk_write(requests, ordered=False)\n",
    "\n",
    "    def reset(self):\n",
    "        db = self.client[self.DATABASE_NAME]\n",
    "        for collection_name in self.collection_names:\n",
    "            db[collection_name].drop()\n",
    "\n",
    "    def getName(self) -> str:\n",
    "        return \"MongoDB\"\n",
    "\n",
    "    def __del__(self):\n",
    "        self.client.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "import requests\n",
    "\n",
    "class PrometheusDatabaseTesting(DatabaseTestingInterface):\n",
    "    def __init__(self, connection_string: str, pushgateway_connection_string: str, csv_file_paths: List[str]):\n",
    "        self.connection_string = connection_string\n",
    "        self.pushgateway_connection_string = pushgateway_connection_string\n",
    "        self.registry = CollectorRegistry()\n",
    "        self.csv_data = {}\n",
    "        self.gauge_names = []\n",
    "        self.gauges = {}\n",
    "\n",
    "        for file_path in csv_file_paths:\n",
    "            gauge_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            self.gauge_names.append(gauge_name)\n",
    "            self.csv_data[gauge_name] = self.__parse_csv_data(file_path)\n",
    "\n",
    "        self.__create_gauges()\n",
    "\n",
    "    def __create_gauges(self):\n",
    "        for gauge_name in self.gauge_names:\n",
    "            self.gauges[gauge_name] = Gauge(f\"{gauge_name}\", f\"Metric for monitoring {gauge_name}\", registry=self.registry)\n",
    "\n",
    "    def __parse_csv_data(self, file_path: str) -> List[float]:\n",
    "        MAX_DATA_READ = 1000\n",
    "        with open(file_path, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"|\")\n",
    "            next(reader)    #column_names\n",
    "            rows = itertools.islice(reader, MAX_DATA_READ)\n",
    "            data = [row[2] for row in rows]\n",
    "        return data\n",
    "\n",
    "    def create(self, rows_created: int = 1, transactions: int = None):\n",
    "        for gauge_name, data in self.csv_data.items():\n",
    "            for i in range(rows_created):\n",
    "                self.gauges[gauge_name].set(data[i])\n",
    "                push_to_gateway(self.pushgateway_connection_string, job=\"prometheus\", registry=self.registry)\n",
    "\n",
    "    def read(self, rows_read: int = 1, transactions: int = None):\n",
    "        for gauge_name in self.gauge_names:\n",
    "            query_url = f\"{self.connection_string}/api/v1/query?query={gauge_name}\"\n",
    "            response = requests.get(query_url)\n",
    "            print(response)\n",
    "            if response.status_code == 200:\n",
    "                value = response.json()[\"data\"][\"result\"][0][\"value\"][1]\n",
    "                #print(f\"{gauge_name}: {value}\")\n",
    "            else:\n",
    "                print(\"Error querying Prometheus database\")\n",
    "\n",
    "    def update(self, rows_updated: int = 1, transactions: int = None):\n",
    "        pass\n",
    "\n",
    "    def delete(self, rows_deleted: int = 1, transactions: int = None):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def getName(self) -> str:\n",
    "        return \"Prometheus\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#prometheusDatabaseTesting = PrometheusDatabaseTesting(\"localhost:9090\", csvFiles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "csvFiles = get_csv_file_paths(\"./csvData\")\n",
    "sqlite = CassandraDatabaseTesting(\"localhost\", csvFiles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "sqlite.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "sqlite.create(1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
